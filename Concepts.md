---
typora-root-url: ../../../workspace
---

# Concepts

----

[[Probability and Statistics|Probability and Statistics]]
[[Maximum Likelihood Estimation|Maximum Likelihood Estimation]]
[[ML Interview Preparation 2019|ML Interview Preparation 2019]]
[[Andrew Ng Course Note|Andrew Ng Course Note]]

[t-SNE](https://www.youtube.com/watch?v=NEaUSP4YerM&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=11):

Project high-dimensional data into low-dimensional but keep original information, like clusters, it’s idea a bit slightly similar to my “Density-Flow Clustering"
[Visualizing Data using t-SNE](http://www.jmlr.org/papers/v9/vandermaaten08a.html). Journal of ML Research 2008

[PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=7): Principal Component Analysis

\- reduce the dimensionality of the data
\-  retaining the variation in the data to the maximum possible

![img](/- Worknote/Machine Learning/_resources/Concepts.resources/unknown_filename.png)
minimize the distance from data points to the project line is equivalent to maximize the distance from projected points to origin, that the maximize the variance.

[LDA](https://www.youtube.com/watch?v=azXCzI57Yfc&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=12): Linear Discriminant Analysis
![img](/- Worknote/Machine Learning/_resources/Concepts.resources/unknown_filename.1.png)

[The Curse of Dimensionality](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#the-curse-of-dimensionality)





----

- Date: 2019-01-02
- Tags: #machineLearning 



